# Melanoma_caseStudy

Project Name: Melanoma Detection Assignment
Description: 
Problem Statement:

Melanoma is a type of skin cancer that occurs when the pigment-producing cells that give colour to the skin become cancerous. Itâ€™s symptoms might include a new, unusual growth or a change in an existing mole. It can occur anywhere on the body. It is type of cancer that can be deadly if not detected early. It accounts for 75% of skin cancer deaths. Our target is to build a CNN based model which can accurately detect Melanoma by evaluating images and alerting dermatologists about its presence. This would result in early detection of the disease and also reduce lot of manual effort required in the diagnosis process.

The first step towards the diagnosis includes visual examination of the skin's affected area. This is followed by skin biopsy, results of which generally takes almost a week or more for the final report followed by Doctor appointment. The target is to reduce the initial process to detect the disease and start it's preliminary medication before final reports are out.

Outcome: 
The target is to predict the type of skin cancer accurately which would result in early start of medication which would reduce the nuumber of deaths caused by skin cancer. Here, the image classification technology is used to detect the type of cancer accurately.

Technologies used: 
It has been build using the following:
-> TensorFlow
-> Keras
-> Augmentor

Dataset:
The Dataset consists of 2357 images of skin cancer types. It contains 9 sub-directories in each train and test subdirectories. The 9 sub-directories contains the images of 9 skin cancer types respectively. Out of the total 2357 images, 2239 images are used for training and remaining 118 for testing. It consists of the images of the following cancaer types:
->Actinic keratosis
->Basal cell carcinoma
->Dermatofibroma
->Melanoma
->Nevus
->Pigmented benign keratosis
->Seborrheic keratosis
->Squamous cell carcinoma
->Vascular lesion

General Information:
The batch size is considered to be 32 and the images are resized to 180*180. Out of the 2239 images from testng dataset, 1792 images are used for training i.e. 80% of 2239 and 447 images for validation.
Along with the above class imbalance is also checked and handled using Augmentor. After using the augmentor the image count turned to 4500 (500 in each).

CNN Architecture Design:
1. Rescalling Layer - To rescale an input in the [0, 255] range to be in the [0, 1] range.
2. Convolutional Layer - Convolutional layers apply a convolution operation to the input, passing the result to the next layer. A convolution converts all the pixels in its receptive field into a single value. For example, if you would apply a convolution to an image, you will be decreasing the image size as well as bringing all the information in the field together into a single pixel.
3. Pooling Layer - Pooling layers are used to reduce the dimensions of the feature maps. Thus, it reduces the number of parameters to learn and the amount of computation performed in the network. The pooling layer summarises the features present in a region of the feature map generated by a convolution layer.
4. Flatten Layer - Flattening is converting the data into a 1-dimensional array for inputting it to the next layer. We flatten the output of the convolutional layers to create a single long feature vector. And it is connected to the final classification model, which is called a fully-connected layer.
5. Dense Layer - The dense layer is a neural network layer that is connected deeply, which means each neuron in the dense layer receives input from all neurons of its previous layer.
6. Activation Function(ReLU) - The rectified linear activation function or ReLU for short is a piecewise linear function that will output the input directly if it is positive, otherwise, it will output zero.The rectified linear activation function overcomes the vanishing gradient problem, allowing models to learn faster and perform better.
7. Activation Function(Softmax) - The softmax function is used as the activation function in the output layer of neural network models that predict a multinomial probability distribution. The main advantage of using Softmax is the output probabilities range. The range will 0 to 1, and the sum of all the probabilities will be equal to one.

Conclusion:
Conclusion 1 > With "adam" optimiser, "SparseCategoricalCrossentropy" loss function for model training and epochs=20, on the dataset following was the outcome:

![image](https://user-images.githubusercontent.com/110419166/232325906-47214a19-dad3-4d51-9289-1aa550757106.png)


Observations: 
* As the number of epochs increase, the training accuracy increases whereas the validation accuracy stall at 50% accuracy in training process.
* As the training loss decreases with epochs the validation loss increases.
* Overall, the validation accuracy was around 50-55% for the model.
* The high training accuracy and low validation accuracy notified is a sign of overfitting.

Conclusion 2 > With "adam" optimiser, "SparseCategoricalCrossentropy" loss function for model training and epochs=20, on the augmented dataset following was the outcome:

![image](https://user-images.githubusercontent.com/110419166/232325935-4c502956-284c-4530-af66-875fb883ee10.png)

Observations:
* The overall accuracy is improved a little than the previous model.
* From the plot it can be observed that the gap between the training accuracy and the validation accuracy is much less now when compared to the initial model.
* This implies that the overfitting of the model is greatly reduced when compared to the previous model but the overall accuracy is not much great.


Conclusion 3 > With "adam" optimiser, "SparseCategoricalCrossentropy" loss function for model training and epochs=30, after using the augmentor dataset following was the outcome:

![image](https://user-images.githubusercontent.com/110419166/232325990-f5840241-3a6a-4cf6-9a39-8b27f7f6f524.png)

Observation: 
* With the increase in the training accuracy over time, where as the validation accuracy also increases.
* As the training loss decreases with epochs the validation loss also decreases.
* The plots show that gap between training accuracy and validation accuracy have decreased significantly from previous model, and it has achieved around 83% accuracy on the validation set.

Finally, **Class rebalancing not only got rid of overfitting it also improved the overall accuracy from 55% to 83% and also reduced the overall loss.**



